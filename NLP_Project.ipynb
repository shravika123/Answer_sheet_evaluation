{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0LbEB67eHmGNsZ6egFphQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shravika123/Answer_sheet_evaluation/blob/main/NLP_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "import spacy"
      ],
      "metadata": {
        "id": "g9X8o_vCNCri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4ed8a86-d016-431a-cec8-c6ed9f3cd225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.cli import download\n",
        "download(\"en_core_web_md\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3wKvTr6NWNX",
        "outputId": "0f3ecc8e-faea-4044-e797-36e67ba3a92f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk;\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvq15zxJK6Hs",
        "outputId": "809fe2af-abd9-4a4e-a04a-d0514b9b71ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "import spacy"
      ],
      "metadata": {
        "id": "QVLh-ibFK-lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
      ],
      "metadata": {
        "id": "QUS5N5qLNkyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "\n",
        "data = newsgroups_train.data\n",
        "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
        "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
        "data = [re.sub(\"\\'\", \"\", sent) for sent in data]"
      ],
      "metadata": {
        "id": "uOOW1zBx1XXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sent_to_words(sentences):\n",
        "   for sentence in sentences:\n",
        "      yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "data_words = list(sent_to_words(data))"
      ],
      "metadata": {
        "id": "Bmv7mSsONpBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZAWb7pLOGiw",
        "outputId": "8619bc2f-d861-456c-f54b-a9eeb89f2964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(texts):\n",
        "   return [[word for word in simple_preprocess(str(doc)) \n",
        "   if word not in stop_words] for doc in texts]\n",
        "def make_bigrams(texts):\n",
        "   return [bigram_mod[doc] for doc in texts]\n",
        "def make_trigrams(texts):\n",
        "   return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "   texts_out = []\n",
        "   for sent in texts:\n",
        "      doc = nlp(\" \".join(sent))\n",
        "      texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "   return texts_out"
      ],
      "metadata": {
        "id": "dxKY3lPaOHAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_words_nostops = remove_stopwords(data_words)\n",
        "# data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "data_words_bigrams = make_bigrams(data_words)\n",
        "nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])\n",
        "data_lemmatized = lemmatization(\n",
        "   data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
        ")"
      ],
      "metadata": {
        "id": "isON4fBSOqpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "texts = data_lemmatized\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "#  mm = gensim.corpora.MmCorpus('wiki_tfidf.mm.bz2')"
      ],
      "metadata": {
        "id": "liEUBcK7OUmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lsi_model = gensim.models.lsimodel.LsiModel(\n",
        "   corpus=corpus, id2word=id2word, num_topics=20,chunksize=100\n",
        ")"
      ],
      "metadata": {
        "id": "ovglCyUMOYAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lsi_model.save('_Lsi.gensim')"
      ],
      "metadata": {
        "id": "X6blQQxiQjAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pattern"
      ],
      "metadata": {
        "id": "ICHW39tGQnX3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3423f2f-62fa-4b9a-f0b7-e28176ef09f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pattern\n",
            "  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.2 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from pattern) (0.16.0)\n",
            "Collecting backports.csv\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Collecting mysqlclient\n",
            "  Downloading mysqlclient-2.1.1.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from pattern) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from pattern) (4.9.2)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 8.3 MB/s \n",
            "\u001b[?25hCollecting pdfminer.six\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 42.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pattern) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from pattern) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from pattern) (3.7)\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 52.6 MB/s \n",
            "\u001b[?25hCollecting cherrypy\n",
            "  Downloading CherryPy-18.8.0-py2.py3-none-any.whl (348 kB)\n",
            "\u001b[K     |████████████████████████████████| 348 kB 55.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from pattern) (2.23.0)\n",
            "Collecting zc.lockfile\n",
            "  Downloading zc.lockfile-2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting cheroot>=8.2.1\n",
            "  Downloading cheroot-9.0.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[K     |████████████████████████████████| 100 kB 9.6 MB/s \n",
            "\u001b[?25hCollecting jaraco.collections\n",
            "  Downloading jaraco.collections-3.8.0-py3-none-any.whl (10 kB)\n",
            "Collecting portend>=2.1.1\n",
            "  Downloading portend-3.1.0-py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.8/dist-packages (from cherrypy->pattern) (9.0.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from cheroot>=8.2.1->cherrypy->pattern) (1.15.0)\n",
            "Collecting jaraco.functools\n",
            "  Downloading jaraco.functools-3.5.2-py3-none-any.whl (7.3 kB)\n",
            "Collecting tempora>=1.8\n",
            "  Downloading tempora-5.1.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2022.6)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Collecting jaraco.classes\n",
            "  Downloading jaraco.classes-3.2.3-py3-none-any.whl (6.0 kB)\n",
            "Collecting jaraco.text\n",
            "  Downloading jaraco.text-3.11.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (5.10.1)\n",
            "Collecting jaraco.context>=4.1\n",
            "  Downloading jaraco.context-4.2.0-py3-none-any.whl (5.0 kB)\n",
            "Collecting autocommand\n",
            "  Downloading autocommand-2.2.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.8/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (2.1.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources->jaraco.text->jaraco.collections->cherrypy->pattern) (3.11.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->pattern) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk->pattern) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->pattern) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk->pattern) (4.64.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from pdfminer.six->pattern) (2.1.1)\n",
            "Collecting cryptography>=36.0.0\n",
            "  Downloading cryptography-38.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 45.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.8/dist-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.21)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->pattern) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->pattern) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->pattern) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->pattern) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from zc.lockfile->cherrypy->pattern) (57.4.0)\n",
            "Building wheels for collected packages: pattern, mysqlclient, python-docx, sgmllib3k\n",
            "  Building wheel for pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332722 sha256=ff77364958e1c709cee91e19fa0f847bd2e1e40f8a285d296a8844a9b4cf68ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/ce/8f/bccc2d04f3a25a5a1dd19165b2855ad3203975f25edd5838d6\n",
            "  Building wheel for mysqlclient (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysqlclient: filename=mysqlclient-2.1.1-cp38-cp38-linux_x86_64.whl size=102404 sha256=dd12318d0adc0bcda428538264f1770420077fcfd2e64c679ad1a9f4116e999c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/e1/84/a6185eaec318899f59a32d393af7729a0719cd93695d71f9a1\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184505 sha256=3db01b0a299862e1c358a46011217fae531eb52b6763e4824bef882c62264182\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/b8/b2/c4c2b95765e615fe139b0b17b5ea7c0e1b6519b0a9ec8fb34d\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=b99567df68775e520057b6d9f7088aa54a89684457287ca3e4c5613287f6e476\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/63/2f/117884c3b19d46b64d3d61690333aa80c88dc14050e269c546\n",
            "Successfully built pattern mysqlclient python-docx sgmllib3k\n",
            "Installing collected packages: jaraco.functools, jaraco.context, autocommand, tempora, jaraco.text, jaraco.classes, zc.lockfile, sgmllib3k, portend, jaraco.collections, cryptography, cheroot, python-docx, pdfminer.six, mysqlclient, feedparser, cherrypy, backports.csv, pattern\n",
            "Successfully installed autocommand-2.2.2 backports.csv-1.0.7 cheroot-9.0.0 cherrypy-18.8.0 cryptography-38.0.4 feedparser-6.0.10 jaraco.classes-3.2.3 jaraco.collections-3.8.0 jaraco.context-4.2.0 jaraco.functools-3.5.2 jaraco.text-3.11.0 mysqlclient-2.1.1 pattern-3.6 pdfminer.six-20221105 portend-3.1.0 python-docx-0.8.11 sgmllib3k-1.0.0 tempora-5.1.0 zc.lockfile-2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcblTKWiRRy8",
        "outputId": "f9da33f7-3e23-4548-9bcb-c1b3ad4e7447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __main__ import *\n",
        "import logging\n",
        "from gensim import corpora, similarities\n",
        "from gensim.models import LsiModel\n",
        "from nltk.corpus import stopwords\n",
        "from pattern.en import sentiment\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "metadata": {
        "id": "7fZLNfpOSeDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"mark scheme 2.txt\") as f1:\n",
        "    mark_all = f1.read()\n",
        "\n",
        "# Upload Student Answer now.\n",
        "with open('ans sheet2.txt') as f2:\n",
        "    ans_all = f2.read()\n",
        "\n",
        "anssheet = ans_all.split('~\\n\\n')\n",
        "anssheet = [d for d in anssheet if d]\n",
        "\n",
        "if len(anssheet) == 0:\n",
        "    print(\"student is absent for the test !\")\n",
        "    exit()\n",
        "\n",
        "markscheme = mark_all.split('~\\n\\n')\n",
        "markscheme = [d for d in markscheme if d]\n",
        "\n",
        "new_list_ans = []\n",
        "new_list_mar = []\n",
        "\n",
        "with open('written_ans.txt') as wa:\n",
        "    fwa = wa.readlines()\n",
        "\n",
        "written_answers = fwa[0].split(',')\n",
        "\n",
        "with open('marks_allot.txt') as ma:\n",
        "    fma = ma.readlines()\n",
        "\n",
        "marks_allotment = []\n",
        "mmm = []\n",
        "\n",
        "for r in fma:\n",
        "    t = r.strip()\n",
        "    t = t.split('-')\n",
        "    marks_allotment.append((t[0], t[1]))\n",
        "    mmm.append(t[1])\n",
        "\n",
        "marks_allotment = dict(marks_allotment)\n",
        "max_marks = 0\n",
        "for i in mmm:\n",
        "    j = int(i)\n",
        "    max_marks = max_marks + j\n",
        "\n",
        "for j in written_answers:\n",
        "    for ans in anssheet:\n",
        "        if ans.startswith(j):\n",
        "            ans = ans[2:]\n",
        "            new_list_ans.append(ans)\n",
        "\n",
        "    for mar in markscheme:\n",
        "        if mar.startswith(j):\n",
        "            mar = mar[2:]\n",
        "            new_list_mar.append(mar)\n",
        "\n",
        "total_marks = 0\n",
        "\n",
        "\n",
        "def marking_scheme(file1):\n",
        "    global documents\n",
        "    documents = file1.split('.')\n",
        "    documents = [d.strip() for d in documents]\n",
        "    documents = [d for d in documents if d]\n",
        "\n",
        "\n",
        "# with open('MarkingScheme.txt') as f2:\n",
        "#    file2 = f2.read()\n",
        "\n",
        "def answer(file2):\n",
        "    global new_documents\n",
        "    new_documents = file2.split('.')\n",
        "    new_documents = [dd.strip() for dd in new_documents]\n",
        "    new_documents = [dd for dd in new_documents if dd]\n",
        "\n",
        "\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "\n",
        "LSI = LsiModel.load('_Lsi.gensim')\n",
        "\n",
        "\n",
        "def pre_process():\n",
        "    global t1, t2, index, len_doc, len_new_doc, len_temp_doc, len_temp_new_doc, dictionary, new_documents, documents, corpus\n",
        "    texts = [[word for word in document.lower().split() if word not in set(stopwords.words('english'))] for document in\n",
        "             documents]\n",
        "    dictionary = corpora.Dictionary(texts)\n",
        "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "    index = similarities.MatrixSimilarity(LSI[corpus])\n",
        "    len_new_doc = [x for x in range(len(new_documents))]\n",
        "    len_doc = [y for y in range(len(documents))]\n",
        "    t1 = [x for x in range(len(new_documents))]\n",
        "    t2 = [y for y in range(len(documents))]\n",
        "    len_temp_new_doc = len_new_doc\n",
        "    len_temp_doc = len_doc\n",
        "\n",
        "\n",
        "def process_sim():\n",
        "    global len_new_doc, sim_array, index, dictionary, new_documents, documents\n",
        "    for sent in len_new_doc[:]:\n",
        "        vec_bow = dictionary.doc2bow(new_documents[sent].lower().split())\n",
        "        vec_lsi = LSI[vec_bow]\n",
        "        sims = index[vec_lsi]\n",
        "        sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
        "        lg = []\n",
        "        for o, p in sims:\n",
        "            if p == 0.0:\n",
        "                lg.append(p)\n",
        "        if lg == [0.0] * len(documents):\n",
        "            t1.remove(sent)\n",
        "            len_new_doc.remove(sent)\n",
        "        else:\n",
        "            sim_array.append(sims)\n",
        "\n",
        "\n",
        "def process_zero():\n",
        "    global len_doc, indx\n",
        "    for c in len_doc:\n",
        "        max_sim = 0\n",
        "        count = -1\n",
        "        for m in sim_array:\n",
        "            count = count + 1\n",
        "            for e, k in m:\n",
        "                if e == c:\n",
        "                    if k > max_sim:\n",
        "                        max_sim = k\n",
        "                        indx = len_new_doc[count]\n",
        "        avg_sim.append((c, indx, max_sim))\n",
        "\n",
        "\n",
        "def process_one():\n",
        "    global len_new_doc, len_temp_new_doc, len_temp_doc, len_doc, final_sim\n",
        "    for n in len_new_doc:\n",
        "        tem = 0\n",
        "        added = None\n",
        "        for a, b, c in avg_sim:\n",
        "            if b == n:\n",
        "                if c > tem:\n",
        "                    tem = c\n",
        "                    added = (a, b, c)\n",
        "        if added is not None:\n",
        "            final_sim.append(added)\n",
        "    len_temp_new_doc = [w for q, w, e in final_sim]\n",
        "    len_temp_doc = [r for r, t, y in final_sim]\n",
        "    len_new_doc = [x for x in t1 if x not in len_temp_new_doc]\n",
        "    len_doc = [y for y in t2 if y not in len_temp_doc]\n",
        "\n",
        "\n",
        "def process_sim1():\n",
        "    global len_doc, sim_array, index, dictionary, new_documents, documents\n",
        "    sim_array = []\n",
        "    for sent in len_new_doc[:]:\n",
        "        vec_bow = dictionary.doc2bow(new_documents[sent].lower().split())\n",
        "        vec_lsi = LSI[vec_bow]\n",
        "        sims = index[vec_lsi]\n",
        "        sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
        "        lg = []\n",
        "        for o, p in sims:\n",
        "            if p == 0.0:\n",
        "                lg.append(p)\n",
        "        if lg == [0.0] * len(documents):\n",
        "            t1.remove(sent)\n",
        "            len_new_doc.remove(sent)\n",
        "        else:\n",
        "            sim_array.append(sims)\n",
        "\n",
        "\n",
        "def process_zero1():\n",
        "    global len_new_doc, indx\n",
        "    for c in len_doc:\n",
        "        max_sim = 0\n",
        "        count = -1\n",
        "        indx = None\n",
        "        for m in sim_array:\n",
        "            count = count + 1\n",
        "            for e, k in m:\n",
        "                if e == c:\n",
        "                    if k > max_sim:\n",
        "                        max_sim = k\n",
        "                        indx = len_new_doc[count]\n",
        "        if indx is not None:\n",
        "            avg_sim.append((indx, c, max_sim))\n",
        "\n",
        "\n",
        "def process_two():\n",
        "    global len_new_doc, len_temp_new_doc, len_temp_doc, len_doc, final_sim, avg_sim\n",
        "    for n in len_new_doc:\n",
        "        temp = 0\n",
        "        added = None\n",
        "        for a, b, c in avg_sim:\n",
        "            if a == n:\n",
        "                if c > temp:\n",
        "                    temp = c\n",
        "                    added = (a, b, c)\n",
        "        if added is not None:\n",
        "            final_sim.append(added)\n",
        "    len_temp_new_doc = [q for q, w, e in final_sim]\n",
        "    len_temp_doc = [t for r, t, y in final_sim]\n",
        "    len_new_doc = [x for x in t1 if x not in len_temp_new_doc]\n",
        "    len_doc = [y for y in t2 if y not in len_temp_doc]\n",
        "\n",
        "# print(len(written_answers))\n",
        "# print(len(new_list_mar))\n",
        "for i in range(len(written_answers)):\n",
        "    temp = new_list_mar[i]\n",
        "    temp1 = new_list_ans[i]\n",
        "    marking_scheme(temp)\n",
        "    answer(temp1)\n",
        "    pre_process()\n",
        "    if len(documents) > len(new_documents):\n",
        "        final_sim = []\n",
        "        while len(len_new_doc) > 0:\n",
        "            sim_array = []\n",
        "            process_sim()\n",
        "            avg_sim = []\n",
        "            process_zero()\n",
        "            process_one()\n",
        "        dop = final_sim\n",
        "        for g, h, l in dop:\n",
        "            if l > 0.80:\n",
        "                s1 = sentiment(documents[g])\n",
        "                s2 = sentiment(new_documents[h])\n",
        "                diff = abs(s1[0] - s2[0])\n",
        "                if s1[0] < 0 and s2[0] < 0:\n",
        "                    pass\n",
        "                elif s1[0] > 0 and s2[0] > 0:\n",
        "                    pass\n",
        "                else:\n",
        "                    if diff > 0.4:\n",
        "                        final_sim.remove((g, h, l))\n",
        "\n",
        "        sum1 = [j for g, h, j in final_sim]\n",
        "        avg = (sum(sum1)) / len(documents)\n",
        "        marks = round(((avg * 100) * int(marks_allotment[written_answers[i]])) / 100)\n",
        "    else:\n",
        "        final_sim = []\n",
        "        while len(len_doc) > 0 and len(len_new_doc) > 0:\n",
        "            sim_array = []\n",
        "            process_sim1()\n",
        "            avg_sim = []\n",
        "            process_zero1()\n",
        "            process_two()\n",
        "        dop = final_sim\n",
        "        for g, h, l in dop:\n",
        "            if l > 0.80:\n",
        "                s1 = sentiment(documents[h])\n",
        "                s2 = sentiment(new_documents[g])\n",
        "                diff = abs(s1[0] - s2[0])\n",
        "                if s1[0] < 0 and s2[0] < 0:\n",
        "                    pass\n",
        "                elif s1[0] > 0 and s2[0] > 0:\n",
        "                    pass\n",
        "                else:\n",
        "                    if diff > 0.4:\n",
        "                        final_sim.remove((g, h, l))\n",
        "        sum1 = [j for g, h, j in final_sim]\n",
        "        avg = (sum(sum1)) / len(documents)\n",
        "        marks = round(((avg * 100) * int(marks_allotment[written_answers[i]])) / 100)\n",
        "\n",
        "    print(\" Question\", written_answers[i], \"=\", marks, \"/\", int(marks_allotment[written_answers[i]]))\n",
        "    total_marks = total_marks + marks\n",
        "\n",
        "kk = marks_allotment.keys()\n",
        "ky = marks_allotment.values()\n",
        "\n",
        "mar = [int(marks_allotment[d]) for d in kk if d in written_answers]\n",
        "print(\"total marks out of\", max_marks, \"=\", total_marks)\n",
        "\n",
        "ttm = sum(mar)\n",
        "summarks = total_marks\n",
        "\n",
        "\n",
        "def get_value():\n",
        "    s = \"marks obtained: \" + str(max_marks) + \" / \" + str(ttm)\n",
        "    return s\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twTFdaWXSexN",
        "outputId": "7916338f-9d6c-43e5-9f98-2f4162f66ef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
            "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
            "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
            "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
            "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Question 1 = 25 / 50\n",
            " Question 2 = 0 / 50\n",
            " Question 3 = 50 / 50\n",
            " Question 4 = 50 / 50\n",
            " Question 5 = 50 / 50\n",
            "total marks out of 250 = 175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y2VOyX9TAacK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}